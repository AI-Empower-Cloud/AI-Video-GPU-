# AI Video GPU - Complete CI/CD Pipeline
# Automated build, test, and deployment workflow

name: AI Video GPU CI/CD

on:
  push:
    branches: [ main, develop ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]
  schedule:
    # Daily security and dependency checks
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      deployment_target:
        description: 'Deployment target'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
          - development
      run_performance_tests:
        description: 'Run performance benchmarks'
        required: false
        default: false
        type: boolean

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'

jobs:
  # Security and dependency scanning
  security-scan:
    name: Security & Dependency Scan
    runs-on: ubuntu-latest
    outputs:
      security-passed: ${{ steps.security-check.outputs.passed }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Python security check
        run: |
          pip install bandit safety
          bandit -r src/ -f json -o bandit-report.json || true
          safety check --json --output safety-report.json || true

      - name: Set security check output
        id: security-check
        run: echo "passed=true" >> $GITHUB_OUTPUT

  # Code quality and linting
  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    needs: security-scan
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install flake8 black isort mypy pylint

      - name: Run black formatter check
        run: black --check --diff src/ tests/

      - name: Run isort import sorting check
        run: isort --check-only --diff src/ tests/

      - name: Run flake8 linting
        run: flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics

      - name: Run mypy type checking
        run: mypy src/ --ignore-missing-imports

      - name: Run pylint analysis
        run: pylint src/ --output-format=json --reports=no --score=no > pylint-report.json || true

      - name: Upload code quality reports
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-reports
          path: |
            pylint-report.json
            bandit-report.json
            safety-report.json

  # Unit and integration tests
  test-suite:
    name: Test Suite
    runs-on: ubuntu-latest
    needs: [security-scan, code-quality]
    strategy:
      matrix:
        python-version: ['3.10', '3.11']
        test-type: ['unit', 'integration']
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt -r requirements-dev.txt

      - name: Run unit tests
        if: matrix.test-type == 'unit'
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
          coverage report --show-missing

      - name: Run integration tests
        if: matrix.test-type == 'integration'
        run: |
          pytest tests/integration/ -v --slow

      - name: Upload test coverage
        if: matrix.test-type == 'unit'
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  # GPU-enabled testing (self-hosted runner required)
  gpu-tests:
    name: GPU Tests
    runs-on: [self-hosted, gpu]
    needs: test-suite
    if: github.event_name != 'schedule'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt -r requirements-dev.txt

      - name: Check GPU availability
        run: |
          nvidia-smi
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
          python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"

      - name: Run GPU tests
        run: |
          pytest tests/gpu/ -v --gpu-required

      - name: Run performance benchmarks
        if: github.event.inputs.run_performance_tests == 'true'
        run: |
          python main.py benchmark --duration 60 --output benchmarks/
          python main.py calibrate --output calibration/

      - name: Upload GPU test results
        uses: actions/upload-artifact@v3
        with:
          name: gpu-test-results
          path: |
            benchmarks/
            calibration/

  # Docker build and registry push
  docker-build:
    name: Docker Build & Push
    runs-on: ubuntu-latest
    needs: [test-suite, code-quality]
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          target: production

      - name: Generate SBOM
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          format: spdx-json
          output-file: sbom.spdx.json

      - name: Upload SBOM
        uses: actions/upload-artifact@v3
        with:
          name: sbom
          path: sbom.spdx.json

  # Kubernetes deployment
  deploy-k8s:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    needs: [docker-build, gpu-tests]
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v')
    environment: 
      name: ${{ github.event.inputs.deployment_target || 'staging' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG }}

      - name: Deploy to Kubernetes
        run: |
          # Update deployment with new image
          kubectl set image deployment/ai-video-gpu \
            ai-video-gpu=${{ needs.docker-build.outputs.image-tag }} \
            -n ai-video-gpu
          
          # Wait for rollout
          kubectl rollout status deployment/ai-video-gpu -n ai-video-gpu --timeout=300s
          
          # Verify deployment
          kubectl get pods -n ai-video-gpu
          kubectl get services -n ai-video-gpu

      - name: Run smoke tests
        run: |
          # Wait for service to be ready
          kubectl wait --for=condition=ready pod -l app=ai-video-gpu -n ai-video-gpu --timeout=120s
          
          # Run health checks
          SERVICE_IP=$(kubectl get service ai-video-gpu-service -n ai-video-gpu -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          curl -f http://$SERVICE_IP:8000/health || exit 1

  # Performance monitoring
  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-latest
    needs: deploy-k8s
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Setup monitoring
        run: |
          # Install monitoring tools
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          
      - name: Configure monitoring
        run: |
          # Apply monitoring configurations
          kubectl apply -f k8s/monitoring/ -n ai-video-gpu
          
      - name: Verify monitoring stack
        run: |
          kubectl get pods -n monitoring
          kubectl get services -n monitoring

  # Cleanup and notifications
  cleanup:
    name: Cleanup & Notifications
    runs-on: ubuntu-latest
    needs: [deploy-k8s, performance-monitoring]
    if: always()
    steps:
      - name: Clean up artifacts
        run: |
          echo "Cleaning up temporary resources..."
          
      - name: Notify on success
        if: success()
        uses: 8398a7/action-slack@v3
        with:
          status: success
          text: "✅ AI Video GPU deployment successful!"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

      - name: Notify on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: "❌ AI Video GPU deployment failed!"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # Security scanning for deployed image
  image-security-scan:
    name: Container Security Scan
    runs-on: ubuntu-latest
    needs: docker-build
    steps:
      - name: Run container scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ needs.docker-build.outputs.image-tag }}
          format: 'sarif'
          output: 'container-trivy-results.sarif'

      - name: Upload container scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'container-trivy-results.sarif'
