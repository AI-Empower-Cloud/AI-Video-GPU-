{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a79860",
   "metadata": {},
   "source": [
    "# AI Video GPU Platform - Kaggle Setup\n",
    "\n",
    "This notebook sets up the AI Video GPU platform on Kaggle with free GPU access.\n",
    "\n",
    "## Prerequisites\n",
    "- Kaggle account (free)\n",
    "- Phone verification for GPU access\n",
    "- Enable \"Internet\" and \"GPU\" in notebook settings\n",
    "\n",
    "## GPU Availability\n",
    "- **Free Tier**: Up to 30 hours/week of GPU time\n",
    "- **GPU Types**: T4 x2, P100 (depending on availability)\n",
    "- **RAM**: 13-16GB\n",
    "- **Storage**: 20GB temporary\n",
    "\n",
    "‚ö†Ô∏è **Important**: Make sure to enable GPU in notebook settings (Accelerator > GPU T4 x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379eda6",
   "metadata": {},
   "source": [
    "## Step 1: System Information & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faec4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "print(\"üîç System Information:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not detected! Please enable GPU in notebook settings.\")\n",
    "    print(\"Go to Settings > Accelerator > GPU T4 x2\")\n",
    "\n",
    "# Check available disk space\n",
    "result = subprocess.run(['df', '-h', '/kaggle/working'], capture_output=True, text=True)\n",
    "print(f\"\\nüíæ Disk space:\\n{result.stdout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b48a34",
   "metadata": {},
   "source": [
    "## Step 2: Install System Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update system packages\n",
    "!apt-get update -qq\n",
    "\n",
    "# Install essential system packages\n",
    "!apt-get install -y -qq \\\n",
    "    ffmpeg \\\n",
    "    libsm6 \\\n",
    "    libxext6 \\\n",
    "    libxrender-dev \\\n",
    "    libglib2.0-0 \\\n",
    "    libgl1-mesa-glx \\\n",
    "    git \\\n",
    "    wget \\\n",
    "    curl\n",
    "\n",
    "print(\"‚úÖ System dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88390251",
   "metadata": {},
   "source": [
    "## Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df80fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Clone the repository\n",
    "if not os.path.exists('AI-Video-GPU'):\n",
    "    !git clone https://github.com/yourusername/AI-Video-GPU.git\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"üìÅ Repository already exists\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/kaggle/working/AI-Video-GPU')\n",
    "print(f\"üìç Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List contents to verify\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe93dc",
   "metadata": {},
   "source": [
    "## Step 4: Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d79255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Install main requirements\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install additional packages for Kaggle environment\n",
    "!pip install gradio pyngrok\n",
    "\n",
    "print(\"‚úÖ Python dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374f84a",
   "metadata": {},
   "source": [
    "## Step 5: Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c88202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables for Kaggle\n",
    "os.environ['KAGGLE_ENV'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Use both GPUs if available\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# Set paths\n",
    "os.environ['PROJECT_ROOT'] = '/kaggle/working/AI-Video-GPU'\n",
    "os.environ['MODEL_CACHE'] = '/kaggle/working/models'\n",
    "os.environ['OUTPUT_DIR'] = '/kaggle/working/outputs'\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('/kaggle/working/models', exist_ok=True)\n",
    "os.makedirs('/kaggle/working/outputs', exist_ok=True)\n",
    "os.makedirs('/kaggle/working/temp', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment configured for Kaggle!\")\n",
    "print(f\"üìÅ Models cache: {os.environ['MODEL_CACHE']}\")\n",
    "print(f\"üìÅ Output directory: {os.environ['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993e54f",
   "metadata": {},
   "source": [
    "## Step 6: Download Models (Optional - Heavy Downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b21b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell downloads AI models. Uncomment and run if needed.\n",
    "# Note: Models are large and may take significant time/bandwidth\n",
    "\n",
    "# import torch\n",
    "# from huggingface_hub import snapshot_download\n",
    "# \n",
    "# print(\"üîÑ Downloading models... This may take several minutes\")\n",
    "# \n",
    "# # Download Stable Diffusion model\n",
    "# try:\n",
    "#     model_path = snapshot_download(\n",
    "#         repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "#         cache_dir=\"/kaggle/working/models\",\n",
    "#         allow_patterns=[\"*.bin\", \"*.json\", \"*.txt\"]\n",
    "#     )\n",
    "#     print(f\"‚úÖ Stable Diffusion downloaded to {model_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Error downloading Stable Diffusion: {e}\")\n",
    "# \n",
    "# # Download other models as needed\n",
    "# # Add more model downloads here\n",
    "\n",
    "print(\"üí° Model download cell ready. Uncomment code above to download models.\")\n",
    "print(\"‚ö†Ô∏è Note: Models are large and will consume significant time and bandwidth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720648d",
   "metadata": {},
   "source": [
    "## Step 7: Set up Ngrok for Public Access (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bf46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set up ngrok for public access\n",
    "# Get your free ngrok token from https://ngrok.com/\n",
    "\n",
    "NGROK_TOKEN = \"\"  # Enter your ngrok token here\n",
    "\n",
    "if NGROK_TOKEN:\n",
    "    from pyngrok import ngrok\n",
    "    \n",
    "    # Set ngrok auth token\n",
    "    ngrok.set_auth_token(NGROK_TOKEN)\n",
    "    print(\"‚úÖ Ngrok configured for public access\")\n",
    "else:\n",
    "    print(\"üí° Ngrok token not provided. App will run locally only.\")\n",
    "    print(\"Get your free token at: https://ngrok.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28e002",
   "metadata": {},
   "source": [
    "## Step 8: Launch the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project to Python path\n",
    "sys.path.append('/kaggle/working/AI-Video-GPU')\n",
    "\n",
    "try:\n",
    "    # Import and launch the main application\n",
    "    from main import create_gradio_interface\n",
    "    \n",
    "    # Create the Gradio interface\n",
    "    demo = create_gradio_interface()\n",
    "    \n",
    "    # Launch with Kaggle-specific settings\n",
    "    if 'NGROK_TOKEN' in globals() and NGROK_TOKEN:\n",
    "        # Launch with ngrok for public access\n",
    "        demo.launch(\n",
    "            share=True,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7860,\n",
    "            debug=True\n",
    "        )\n",
    "    else:\n",
    "        # Launch locally (Kaggle internal only)\n",
    "        demo.launch(\n",
    "            share=False,\n",
    "            server_name=\"0.0.0.0\",\n",
    "            server_port=7860,\n",
    "            debug=True,\n",
    "            inline=True  # Show inline in Kaggle\n",
    "        )\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Running basic demo instead...\")\n",
    "    \n",
    "    # Fallback: Basic Gradio demo\n",
    "    import gradio as gr\n",
    "    \n",
    "    def basic_demo(text):\n",
    "        return f\"AI Video GPU Platform is running! Input: {text}\"\n",
    "    \n",
    "    demo = gr.Interface(\n",
    "        fn=basic_demo,\n",
    "        inputs=gr.Textbox(label=\"Test Input\"),\n",
    "        outputs=gr.Textbox(label=\"Output\"),\n",
    "        title=\"AI Video GPU - Kaggle Demo\",\n",
    "        description=\"Basic demo running on Kaggle GPU environment\"\n",
    "    )\n",
    "    \n",
    "    demo.launch(\n",
    "        share=False,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        inline=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453af5ed",
   "metadata": {},
   "source": [
    "## Step 9: Test GPU Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"üöÄ Testing GPU performance...\")\n",
    "    \n",
    "    # Test matrix multiplication on GPU\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Create large tensors\n",
    "    size = 4096\n",
    "    a = torch.randn(size, size, device=device)\n",
    "    b = torch.randn(size, size, device=device)\n",
    "    \n",
    "    # Warm up\n",
    "    _ = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    result = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚úÖ GPU matrix multiplication ({size}x{size}): {end_time - start_time:.3f} seconds\")\n",
    "    print(f\"üî• GPU memory used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    print(f\"üî• GPU memory cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del a, b, result\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå GPU not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b4455",
   "metadata": {},
   "source": [
    "## Usage Tips for Kaggle\n",
    "\n",
    "### üìù Important Notes:\n",
    "1. **GPU Quota**: You have 30 hours/week of free GPU time\n",
    "2. **Session Limits**: Sessions auto-terminate after 9 hours of inactivity\n",
    "3. **Storage**: 20GB temporary storage, data is lost when session ends\n",
    "4. **Internet**: Must be enabled in notebook settings for downloads\n",
    "\n",
    "### üí° Best Practices:\n",
    "- Save important outputs to Kaggle Datasets for persistence\n",
    "- Monitor GPU usage in the right panel\n",
    "- Use smaller models for quick testing\n",
    "- Enable version control to save notebook states\n",
    "\n",
    "### üîß Troubleshooting:\n",
    "- **No GPU**: Enable GPU in Settings ‚Üí Accelerator\n",
    "- **Out of Memory**: Reduce batch sizes or use gradient checkpointing\n",
    "- **Slow Downloads**: Large models may timeout, try smaller variants\n",
    "- **Session Timeout**: Re-run cells to restore state\n",
    "\n",
    "### üìä Monitoring Resources:\n",
    "```python\n",
    "# Check GPU memory\n",
    "!nvidia-smi\n",
    "\n",
    "# Check disk usage\n",
    "!df -h\n",
    "\n",
    "# Check RAM usage\n",
    "!free -h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce3b39",
   "metadata": {},
   "source": [
    "## üéâ Setup Complete!\n",
    "\n",
    "Your AI Video GPU platform is now running on Kaggle! \n",
    "\n",
    "### Next Steps:\n",
    "1. Access the Gradio interface above\n",
    "2. Upload your media files\n",
    "3. Configure generation settings\n",
    "4. Generate AI videos and content\n",
    "\n",
    "### Save Your Work:\n",
    "- Use \"Save Version\" to preserve your notebook\n",
    "- Export outputs to Kaggle Datasets\n",
    "- Download important files before session ends\n",
    "\n",
    "Happy creating! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
